{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "45718ddbdacc17ac",
      "metadata": {
        "id": "45718ddbdacc17ac"
      },
      "source": [
        "# **Tarea 2, Aprendizaje en entornos complejos**\n",
        "\n",
        "## **Autores:** Ana Gil Molina, José María García Ortiz y Levi Malest Villarreal\n",
        "\n",
        "<br>\n",
        "\n",
        "**Description:** Esta notebook plantea una introducción al problema de aprendizaje en entornos complejos, y da acceso a los diferentes estudios que se realizan sobre este problema, a través de links a las notebooks donde se desarrollan (véase sección **[2]**). El repositorio que contiene los ficheros necesarios para desarrollar este trabajo está accesible de forma pública en GitHub mediante el siguiente enlace: [consultar repositorio](https://github.com/JMGO-coding/RL_GGM).\n",
        "\n",
        "<br>\n",
        "\n",
        "*This software is licensed under the GNU General Public License v3.0 (GPL-3.0),\n",
        "with the additional restriction that it may not be used for commercial purposes.*\n",
        "\n",
        "*For more details about GPL-3.0: https://www.gnu.org/licenses/gpl-3.0.html*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c1697e197fa5a08",
      "metadata": {
        "id": "7c1697e197fa5a08"
      },
      "source": [
        "## **[1] - Introducción al problema del *aprendizaje en entornos complejos***\n",
        "\n",
        "Cada toma de decisiones se desarrolla en un contexto. Una forma de abordar el problema es mediante bandidos contextuales. Otra forma, la más habitual, es\n",
        "considerar que las decisiones se rigen por un proceso de decisión de Markov con recompensas (que podrían ser estocásticas). La forma más correcta de resolver este problema es mediante las ecuaciones de Bellman que se pueden resolver mediante cálculo matricial o cálculo iterativo, para lo que se supone\n",
        "que conoce todo (la política correcta, la matriz de transiciones y las recompensas).\n",
        "\n",
        "Pero lo cierto es que en este tipo de problemas no se suele conocer el modelo que rige el entorno ni se sabe que recompensas nos podemos encontrar y mucho menos cuál es la mejor decisión que se debería de tomar en un estado. Para solucionar el problema se recurre a la experiencia del agente cuando\n",
        "transita por el entorno. Así surge un grupo de técnicas que se basan en aplicar la siguiente secuencia de pasos:\n",
        "1. El agente observa el estado actual del entorno.\n",
        "2. Basándose en este estado, el agente toma una acción (de acuerdo a una política) si no está en\n",
        "un estado terminal.\n",
        "3. El entorno cambia a un nuevo estado como resultado de la acción.\n",
        "4. El agente recibe una recompensa que se usará para evaluar la acción tomada.\n",
        "5. Se vuelve al paso 1.\n",
        "\n",
        "Las técnicas de Monte Carlo esperan a que el agente llegue al estado terminal para considerar el episodio y a partir de él actualiza su política (la toma de decisiones en cada estado) para maximizar la recompensa acumulativa futura. Las técnicas de Diferencias Temporales van actualizando la política confirme va tomando decisiones, sin esperar a llegar a un estado terminal. En medio están las que consideran solo algunas primeras recompensas del episodio. Estas técnicas se basan en usar una representación tabular de las funciones de evaluación de estados y acciones.\n",
        "\n",
        "En ocasiones la cantidad de estados y acciones es tan enorme, que no queda más remedio que recurrir a funciones aproximadas de las funciones de evaluación, lo que da lugar a todo un conjunto de técnicas basadas en el método del descenso del gradiente (como el uso de redes neuronales). Alternativamente, otro grupo de técnicas se enfocan en optimizar la política directamente lo que da lugar a los métodos Actor-Crítico."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e67de1a19a3698f",
      "metadata": {
        "id": "4e67de1a19a3698f"
      },
      "source": [
        "## **[2] - Enlaces a los experimentos realizados sobre este problema**\n",
        "\n",
        "- [Estudio sobre el entorno Frozen Lake](https://colab.research.google.com/github/JMGO-coding/RL_GGM/blob/main/estudio_FrozenLake.ipynb)\n",
        "\n",
        "- [Estudio sobre el entorno Taxi](https://colab.research.google.com/github/JMGO-coding/RL_GGM/blob/main/estudio_Taxi.ipynb)\n",
        "\n",
        "- [Estudio sobre el entorno Mountain Car](https://colab.research.google.com/github/JMGO-coding/RL_GGM/blob/main/estudio_MountainCar.ipynb)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}